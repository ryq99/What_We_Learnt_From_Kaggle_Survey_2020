{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_rows', 5000)\npd.set_option('display.max_columns', 5000)\npd.set_option('display.width', 1000)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\n\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n</script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# we load 2020 data, and later we will create a dense format data which is easier to aggregate \nk_2020 = pd.read_csv(r'/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv')\nk_2020_adj = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now that I say Kaggle Survey is biased, are you surprised 🙀? I'm sorry but truly, there is nothing wrong with the survey itself, it's just no survey is flawless. Here are some reasons:\n* The survey sample is the Kaggle user. And the Kaggle users are active learners. They 📙, 📝, ⌨️ and they 🌱🔜🌿🔜🌻. They may locate in the upper quartile of the population and don't represent the true 📊 of interest, data scientists, data engineers, you name it.\n* Say what if we are in the perfect 🌎? Nah it's not gonna happen, and you know, not everybody has the time or even willingness to finish a survey with 39 choices, 39 😨??!! Yes it is.\n* Well, and a lot of them are multiple selections, ok, please select all that apply so that we can get better data 🙌🙌🙌! But w8, I don't find my choice then what should I do now?\n* Besides, sometimes people would like to take the survey but they just wanna get the shxt done ASAP. Curious how fast? Like really really fast, like 🏃, 🐆, 🏎️, 🚅, 🛫 (and it's shown below)."},{"metadata":{},"cell_type":"markdown","source":"## However, the Kaggle Survey has so much information and aspects we can explore! It's truly awesome! Yea I know I said it's biased, but maybe I'm biased too, and at least we can explore the following two points:\n* What Kagglers think important. Believe me or not, it's the truth that what human see and choose is what they think important to them. \n* Kagglers' growth mindset, from 🌱 to 🌻. It's the 🔑 to data scientists, and the only path to anything and anybody right? "},{"metadata":{},"cell_type":"markdown","source":"## Here are the 39 Survey Questions:\n#### (It took me 28 scrolls to the bottom lol, test yours, and don't forget to read the questions!)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# collect all questions and descriptions into a dictionary\nquestions = dict(zip(k_2020.columns.tolist(), k_2020.iloc[0, :].tolist()))\n\n# better print thess questions as future reference\nfor question, description in questions.items():\n    print('{}  ===>  {} \\n'.format(question, description))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"k_2020 = k_2020.iloc[1:, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q0. Time from Start to Finish (seconds)\n#k_2020['Time from Start to Finish (seconds)'].astype(int).hist()\nk_2020_adj['t_survey'] = k_2020['Time from Start to Finish (seconds)'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q1. What is your age?\n#k_2020['Q1'].value_counts()\nk_2020_adj['age'] = k_2020['Q1'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q2. What is your gender?\n#k_2020['Q2'].value_counts()\nk_2020_adj['gender'] = k_2020['Q2'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q3. In which country do you currently reside?\n#k_2020['Q3'].value_counts()\nk_2020_adj['country'] = k_2020['Q3'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q4. What is the highest level of formal education that you have attained or plan to attain within the next 2 years?\n#k_2020['Q4'].value_counts()\nk_2020_adj['formal_edu'] = k_2020['Q4'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q5. Select the title most similar to your current role (or most recent title if retired)\n#k_2020['Q5'].value_counts()\nk_2020_adj['job_title'] = k_2020['Q5'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q6. For how many years have you been writing code and/or programming?\nk_2020['Q6'].value_counts()\nk_2020_adj['code_exp'] = k_2020['Q6'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q7. What programming languages do you use on a regular basis? (Select all that apply)\nq7_cols = []\nfor q in questions.keys():\n    if 'Q7' in q:\n        q7_cols.append(q)\n#print(q7_cols)\n#k_2020[q7_cols].head()\n\nk_2020_adj['language'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q7_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q8. What programming language would you recommend an aspiring data scientist to learn first?\n#k_2020['Q8'].value_counts()\nk_2020_adj['rec_learn'] = k_2020['Q8'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q9. Which of the following integrated development environments (IDE's) do you use on a regular basis? (Select all that apply)\nq9_cols = []\nfor q in questions.keys():\n    if 'Q9' in q:\n        q9_cols.append(q)\n#print(q9_cols)\n#k_2020[q9_cols].head()\n\nk_2020_adj['ide'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q9_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q10. Which of the following hosted notebook products do you use on a regular basis? (Select all that apply)\nq10_cols = []\nfor q in questions.keys():\n    if 'Q10' in q:\n        q10_cols.append(q)\n#print(q10_cols)\n#k_2020[q10_cols].head()\n\nk_2020_adj['notebook'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q10_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q11. What type of computing platform do you use most often for your data science projects?\n#k_2020['Q11'].value_counts()\n\nk_2020_adj['comp_plfm'] = k_2020['Q11'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q12. Which types of specialized hardware do you use on a regular basis? (Select all that apply)\nq12_cols = []\nfor q in questions.keys():\n    if 'Q12' in q:\n        q12_cols.append(q)\n#print(q12_cols)\n#k_2020[q12_cols]\n\nk_2020_adj['hardware'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q12_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q13. Approximately how many times have you used a TPU (tensor processing unit)?\n#k_2020['Q13'].value_counts()\n\nk_2020_adj['num_use_TPU'] = k_2020['Q13'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q14. What data visualization libraries or tools do you use on a regular basis? (Select all that apply)\nq14_cols = []\nfor q in questions.keys():\n    if 'Q14' in q:\n        q14_cols.append(q)\n#print(q14_cols)\n#k_2020[q14_cols].head()\n\nk_2020_adj['lib_visual'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q14_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q15. For how many years have you used machine learning methods?\n#k_2020['Q15'].value_counts()\nk_2020_adj['ml_exp'] = k_2020['Q15'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q16. Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply)\nq16_cols = []\nfor q in questions.keys():\n    if 'Q16' in q:\n        q16_cols.append(q)\n#print(q16_cols)\n#k_2020[q16_cols].head()\n\nk_2020_adj['ml_framework'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q16_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q17. Which of the following ML algorithms do you use on a regular basis? (Select all that apply):\nq17_cols = []\nfor q in questions.keys():\n    if 'Q17' in q:\n        q17_cols.append(q)\n#print(q17_cols)\n#k_2020[q17_cols].head()\n\nk_2020_adj['ml_alg'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q17_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q18. Which categories of computer vision methods do you use on a regular basis? (Select all that apply)\n#print('Q18 were asked given Q17\\n')\nq18_cols = []\nfor q in questions.keys():\n    if 'Q18' in q:\n        q18_cols.append(q)\n#print(q18_cols)\n#k_2020[q18_cols].head()\n\nk_2020_adj['cv'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q18_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q19. Which of the following natural language processing (NLP) methods do you use on a regular basis? (Select all that apply)\n#print('Q19 were asked given Q17\\n')\nq19_cols = []\nfor q in questions.keys():\n    if 'Q19' in q:\n        q19_cols.append(q)\n#print(q19_cols)\n#k_2020[q19_cols].head()\n\nk_2020_adj['nlp'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q19_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q20. What is the size of the company where you are employed?\n#k_2020['Q20'].value_counts()\nk_2020_adj['employer_size'] = k_2020['Q20']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q21. Approximately how many individuals are responsible for data science workloads at your place of business?\n#k_2020['Q21'].value_counts()\nk_2020_adj['bus_ds_size'] = k_2020['Q21']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q22. Does your current employer incorporate machine learning methods into their business?\n#k_2020['Q22'].value_counts()\nk_2020_adj['employer_ml_stage'] = k_2020['Q22']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q23. Select any activities that make up an important part of your role at work: (Select all that apply)\nq23_cols = []\nfor q in questions.keys():\n    if 'Q23' in q:\n        q23_cols.append(q)\n#print(q23_cols)\n#k_2020[q23_cols].head()\n\nk_2020_adj['jd'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q23_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q24. What is your current yearly compensation (approximate $USD)?\nk_2020['Q24'].value_counts()\nk_2020_adj['annual_comp'] = k_2020['Q24'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q25. Approximately how much money have you (or your team) spent on machine learning and/or cloud computing services at home (or at work) in the past 5 years (approximate $USD)?\nk_2020['Q25'].value_counts()\nk_2020_adj['ml_cost_prev_5_year'] = k_2020['Q25']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q26-A. Which of the following cloud computing platforms do you use on a regular basis? (Select all that apply)\nq26_a_cols = []\nfor q in questions.keys():\n    if 'Q26_A' in q:\n        q26_a_cols.append(q)\n#print(q26_a_cols)\n#k_2020[q26_a_cols].head()\n\nk_2020_adj['cloud_plfm'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q26_a_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q27-A. Do you use any of the following cloud computing products on a regular basis? (Select all that apply)\n#print('Q27-A were asked given Q26-A\\n')\nq27_a_cols = []\nfor q in questions.keys():\n    if 'Q27_A' in q:\n        q27_a_cols.append(q)\n#print(q27_a_cols)\n#k_2020[q27_a_cols].head()\n\nk_2020_adj['cloud_compute'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q27_a_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q28-A. Do you use any of the following machine learning products on a regular basis? (Select all that apply)\n#print('Q28-A were asked given Q26-A\\n')\nq28_a_cols = []\nfor q in questions.keys():\n    if 'Q28_A' in q:\n        q28_a_cols.append(q)\n#print(q28_a_cols)\n#k_2020[q28_a_cols].head()\n\nk_2020_adj['cloud_ml'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q28_a_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q29-A. Which of the following big data products \n# (relational databases, data warehouses, data lakes, or similar) do you use on a regular basis? (Select all that apply)\n\nq29_a_cols = []\nfor q in questions.keys():\n    if 'Q29_A' in q:\n        q29_a_cols.append(q)\n#print(q29_a_cols)\n#k_2020[q29_a_cols].head()\n\nk_2020_adj['big_data'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q29_a_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q30. Which of the following big data products \n# (relational database, data warehouse, data lake, or similar) do you use most often?\n#print('Q30 were asked given Q29-A\\n')\n#k_2020['Q30'].value_counts()\nk_2020_adj['big_data_tool'] = k_2020['Q30'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q31-A. Which of the following business intelligence tools do you use on a regular basis? (Select all that apply)\nq31_a_cols = []\nfor q in questions.keys():\n    if 'Q31_A' in q:\n        q31_a_cols.append(q)\n#print(q31_a_cols)\n#k_2020[q31_a_cols].head()\n\nk_2020_adj['bus_intel'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q31_a_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q32. Which of the following business intelligence tools do you use most often?\n#print('Q32 were asked given Q31-A\\n')\n#k_2020['Q32'].value_counts()\nk_2020_adj['bus_intel_tool'] = k_2020['Q32'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q33-A. Do you use any automated machine learning tools (or partial AutoML tools) on a regular basis? (Select all that apply)\nq33_a_cols = []\nfor q in questions.keys():\n    if 'Q33_A' in q:\n        q33_a_cols.append(q)\n#print(q33_a_cols)\n#k_2020[q33_a_cols].head()\n\nk_2020_adj['auto_ml'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q33_a_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q34-A. Which of the following automated machine learning tools (or partial AutoML tools) do you use on a regular basis? (Select all that apply)\n#print('Q34-A were asked given Q33-A\\n')\n\nq34_a_cols = []\nfor q in questions.keys():\n    if 'Q34_A' in q:\n        q34_a_cols.append(q)\n#print(q34_a_cols)\n#k_2020[q34_a_cols].head()\n\nk_2020_adj['auto_ml_tool'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q34_a_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q35-A. Do you use any tools to help manage machine learning experiments? (Select all that apply)\nq35_a_cols = []\nfor q in questions.keys():\n    if 'Q35_A' in q:\n        q35_a_cols.append(q)\n#print(q35_a_cols)\n#k_2020[q35_a_cols].head()\n\nk_2020_adj['ml_exp_mngmt'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q35_a_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q36. Where do you publicly share or deploy your data analysis or machine learning applications? (Select all that apply)\nq36_cols = []\nfor q in questions.keys():\n    if 'Q36' in q:\n        q36_cols.append(q)\n#print(q36_cols)\n#k_2020[q36_cols].head()\n\nk_2020_adj['share_plfm'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q36_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q37. On which platforms have you begun or completed data science courses? (Select all that apply)\nq37_cols = []\nfor q in questions.keys():\n    if 'Q37' in q:\n        q37_cols.append(q)\n#print(q37_cols)\n#k_2020[q37_cols].head()\n\nk_2020_adj['ds_course_plfm'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q37_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q38. What is the primary tool that you use at work or school to analyze data? (Include text response)\n#k_2020['Q38'].value_counts()\nk_2020_adj['prime_analytical_tool'] = k_2020['Q38'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q39. Who/what are your favorite media sources that report on data science topics? (Select all that apply)\nq39_cols = []\nfor q in questions.keys():\n    if 'Q39' in q:\n        q39_cols.append(q)\n#print(q39_cols)\n#k_2020[q39_cols].head()\n\nk_2020_adj['ds_media'] = k_2020.apply(lambda x: '|'.join([x[col].strip() for col in q39_cols if str(x[col]) != 'nan']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q26-B.Which of the following cloud computing platforms do you hope to become more familiar with in the next 2 years?\nq26_b_cols = []\nfor q in questions.keys():\n    if 'Q26_B' in q:\n        q26_b_cols.append(q)\n#print(q26_b_cols)\n#k_2020[q26_b_cols].head()\n\nq26_b = k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')].apply(lambda x: '|'.join([x[col].strip() for col in q26_b_cols if str(x[col]) != 'nan']), axis=1)\n\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | \n                         (x['ml_cost_prev_5_year'] == '$0 ($USD)'), \n               'cloud_plfm'] = q26_b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q27-B. In the next 2 years, do you hope to become more familiar with any of these specific cloud computing products? (Select all that apply)\nq27_b_cols = []\nfor q in questions.keys():\n    if 'Q27_B' in q:\n        q27_b_cols.append(q)\n#print(q27_b_cols)\n#k_2020[q27_b_cols].head()\n\nq27_b = k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')].apply(lambda x: '|'.join([x[col].strip() for col in q27_b_cols if str(x[col]) != 'nan']), axis=1)\n\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | \n                         (x['ml_cost_prev_5_year'] == '$0 ($USD)'), \n               'cloud_compute'] = q27_b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q28-B. In the next 2 years, do you hope to become more familiar with any of these specific machine learning products? (Select all that apply)\nq28_b_cols = []\nfor q in questions.keys():\n    if 'Q28_B' in q:\n        q28_b_cols.append(q)\n#print(q28_b_cols)\n#k_2020[q28_b_cols].head()\n\nq28_b = k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')].apply(lambda x: '|'.join([x[col].strip() for col in q28_b_cols if str(x[col]) != 'nan']), axis=1)\n\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | \n                         (x['ml_cost_prev_5_year'] == '$0 ($USD)'), \n               'cloud_ml'] = q28_b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q29-B. Which of the following big data products \n# (relational databases, data warehouses, data lakes, or similar) \n# do you hope to become more familiar with in the next 2 years? (Select all that apply)\nq29_b_cols = []\nfor q in questions.keys():\n    if 'Q29_B' in q:\n        q29_b_cols.append(q)\n#print(q29_b_cols)\n#k_2020[q29_b_cols].head()\n\nq29_b = k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')].apply(lambda x: '|'.join([x[col].strip() for col in q29_b_cols if str(x[col]) != 'nan']), axis=1)\n\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | \n                         (x['ml_cost_prev_5_year'] == '$0 ($USD)'), \n               'big_data'] = q29_b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q31-B. Which of the following business intelligence tools do you hope to become more familiar with in the next 2 years? (Select all that apply)\nq31_b_cols = []\nfor q in questions.keys():\n    if 'Q31_B' in q:\n        q31_b_cols.append(q)\n#print(q31_b_cols)\n#k_2020[q31_b_cols].head()\n\nq31_b = k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')].apply(lambda x: '|'.join([x[col].strip() for col in q31_b_cols if str(x[col]) != 'nan']), axis=1)\n\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | \n                         (x['ml_cost_prev_5_year'] == '$0 ($USD)'), \n               'bus_intel'] = q31_b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q33-B. Which categories of automated machine learning tools (or partial AutoML tools) \n# do you hope to become more familiar with in the next 2 years? (Select all that apply)\nq33_b_cols = []\nfor q in questions.keys():\n    if 'Q33_B' in q:\n        q33_b_cols.append(q)\n#print(q33_b_cols)\n#k_2020[q33_b_cols].head()\n\nq33_b = k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')].apply(lambda x: '|'.join([x[col].strip() for col in q33_b_cols if str(x[col]) != 'nan']), axis=1)\n\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | \n                         (x['ml_cost_prev_5_year'] == '$0 ($USD)'), \n               'auto_ml'] = q33_b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q34-B. Which specific automated machine learning tools (or partial AutoML tools) \n# do you hope to become more familiar with in the next 2 years? (Select all that apply)\nq34_b_cols = []\nfor q in questions.keys():\n    if 'Q34_B' in q:\n        q34_b_cols.append(q)\n#print(q34_b_cols)\n#k_2020[q34_b_cols].head()\n\nq34_b = k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')].apply(lambda x: '|'.join([x[col].strip() for col in q34_b_cols if str(x[col]) != 'nan']), axis=1)\n\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | \n                         (x['ml_cost_prev_5_year'] == '$0 ($USD)'), \n               'auto_ml_tool'] = q34_b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Q35-B. In the next 2 years, \n# do you hope to become more familiar with any of these tools for managing ML experiments? (Select all that apply)\nq35_b_cols = []\nfor q in questions.keys():\n    if 'Q35_B' in q:\n        q35_b_cols.append(q)\n#print(q35_b_cols)\n#k_2020[q35_b_cols].head()\n\nq35_b = k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')].apply(lambda x: '|'.join([x[col].strip() for col in q35_b_cols if str(x[col]) != 'nan']), axis=1)\n\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | \n                         (x['ml_cost_prev_5_year'] == '$0 ($USD)'), \n               'ml_exp_mngmt'] = q35_b","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Like I promised to show how fast a Kaggler can be to finish a survey, here is the answer: 20 seconds\n* I think they quitted the survey in half becuase 20/39 = 0.51s/question is impossible.\n* Finish time that is too short won't be considered, you know, the time when we choose the answers before thinking, jump around and leave them blank, and treat the survey just as getting a trivial thing done. The 😎 feels not right.\n* So here we make an assumption that to make sure the survey validity, the minimal time of reading/answering a question in second is 5, total is expected to be 5 * 39 = 195 seconds"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# time in minutes\n# assume avg. min time in second per question is 5, total is expected to be 5 * 35 = 175 seconds\nk_2020 = k_2020.rename(columns={'Time from Start to Finish (seconds)': 't_survey'})\nk_2020['t_survey'] = k_2020['t_survey'].astype(int)\n\n#print(k_2020['t_survey'].describe())\n\nplt.figure(figsize=(8, 4))\nplt.xlim(0, 400)\n(k_2020.loc[lambda x: x['t_survey'] <= 400]['t_survey']).hist(bins=30)\nplt.title('A Closer Look at Survey Time < 400s', fontsize=15)\nplt.axvline(x=195, ymax=0.565, color='red')\nplt.text(x=200, y=130, s='threshold @195s', color='red', fontsize='medium', fontfamily='fantasy', fontweight='semibold')\n\nk_2020_adj = k_2020_adj.loc[lambda x: x['t_survey'] > 195]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# just make a copy \ndf = k_2020_adj.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check out how the data in the dense format look like:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Now we have {:d} surveys for further analysis.'.format(df.shape[0]))\nprint('Our dense data looks like this:')\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Before we jump into the following analysis, think about these two questions again 🤔:\n* **Things Kagglers think important**\n* **Kagglers' growth mindset**"},{"metadata":{},"cell_type":"markdown","source":"# Take-aways and Actions 🔑🔑🔑\n\n### Growth 🚀🚀🚀\n* Focus on learning new things in the first 4 years of career. Find your job needs, build up and stick to 4-5 skills as your core 🤘🏼🤘🏼🤘🏼 to make yourself unique and standout. But don't forget to be open-minded and have some knowledge on other things\n* Engage with the community and interact with others by sharing your work and learning from others, it is all-time important!\n\n### Important Things To Note ⭐⭐⭐\n* Having at least a Master's degree is important for data-related jobs, but it's becoming less important\n* Know Python 🐍, learn 🐍, master 🐍, and integrate 🐍 with the other languages you regularly use to build a network 🕸️ in your brain 🧠 \n* Free services like Google Colab and Kaggle Notebook are the must to know, but don't completely ignore pay services like Amazon Sagemaker because pay 💰 means better experiences and better quality, and it might be closer to the industry needs\n* Visalization is intended to be used to deliver people's thoughts, which is beyond being fancy. It could be simple or complicate and it could be built using any tool/packages (Tableau|Matplotlib|Seaborn|Ggplot|...). However, it must be easy to understand and interactive ✨. "},{"metadata":{},"cell_type":"markdown","source":"### Here I collect my findings by reading through the analysis. Help me check it out, tell me what you think and whether they are ✔️ or ❌ with your understandings. \n* People age from 18 to 34 act as more active learners. More specifically, once they dive into a new area or career, they grow and learn really fast in the first 5 years.\n* Data Scientists market lowers the bar of higher formal education (Master's or Doctor's) and become more open to Bachelor's degrees. However, Data Scientists with Master's degrees still take up the majority part of the market, which is 51.14% in 2020.\n* The most regularly used language is still Python 🐍 and it's the one Kagglers think most important, no matter if they use it. People tend to use python with other one or two languages as a language combo. Popular combos are (Python|SQL), (Python|R), (Python|SQL|R), and (Python|C|C++). However, what's interesting is that for analytics (people using one of Python, R, or SQL), some think Python is very important but some think it is not at all.\n* Popular two notebook platforms are Google Colab and Kaggle Notebook (ps. this is Kaggle survey anyway 🤷🏼). Both the experienced and students like to use them 🆓! As for Amazon Sagemaker, compared with students who almost don't use it at all, a lot more experienced people use it 💰. \n* Data Scientists' favorite visualization tools are Seaborn | Ggplot | Plotly | Shiny | Bokeh | Leaflet/Folium. These packages have things in common: easthetical, easy-to-use, interactive. Especially for plotly ✨, comparing with other jobs, data scientists have more than 10% of people using it.\n* For machine learning and deep learning techniques in general, on average, Kagglers gain 1 skill/working year and tend to become stable after 4-5 years working experience. For more advanced techniques like Computer Vision or Natural Language Processing, people learn 1-2 skills in the first 5 years and stick with it across the career.\n* Job responsibilities can be categorized into three: analytics, machine learning R&D, and pipeline/architecture. Jobs define which job responsibility you should treat more importantly. However, if you are data scientists, machine learning engineers, or research scientists, you will take multiple responsibilities. \n* The cost on Machine learning and cloud services is positively correlated to the company size 🏦, the data science team size 👥, and the company's stage of machine learning developements 🌘🌗🌒. More specifically, compared with other jobs, data scientists have the highest willingness to spend money on these services.\n* Top1 platform to share work is Github. Machine learning engineers, data scientists, and data engineers have higher willingness to share work on platforms while students and the unemployeed don't share work at all. \n* The top3 course platforms are Coursera, Kaggle Learn Course, and Udemy, having 18%, 12.5%, and 12% of users, respectively. Cloud-certification Programs and Fast.ai are the two platforms having least users, which is only 2.5%.\n* The top3 media sources are Kaggle, Youtube, and blogs. Students prefer the more entertaining media type like Youtube while the experienced prefer various of readable and interactive media like Kaggle and blogs."},{"metadata":{},"cell_type":"markdown","source":"## Some Kagglers' basic attributes are 🎂, 🎓️, 📍"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nage = df['age'].value_counts(normalize=True).reset_index().rename(columns={'index':'age', 'age':'ratio'})\nage['age_order'] = age['age'].replace({'18-21':1, '22-24':2, '25-29':3, '30-34':4, '35-39':5, '40-44':6, '45-49':7, '50-54':8, '55-59':9, '60-69':10, '70+':11})\n\nsns.barplot(data=age.sort_values(by=['age_order']), y='age', x='ratio', orient='h')\nplt.title('Ratio of Users - Age', fontsize=15)\nplt.xlabel('ratio')\nplt.ylabel('age')\nprint(\"Most people's ages range from 18 to 34.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"formal_edu = df['formal_edu'].value_counts(normalize=True).reset_index().rename(columns={'index':'formal_edu', 'formal_edu':'ratio'})\nformal_edu['edu_order'] = formal_edu['formal_edu'].replace({'I prefer not to answer':0, \n                                                            'No formal education past high school':1,\n                                                            'Some college/university study without earning a bachelor’s degree':2,\n                                                            'Professional degree':3,\n                                                            'Bachelor’s degree':4,\n                                                            'Master’s degree':5,\n                                                            'Doctoral degree':6})\nplt.figure(figsize=(8, 4))\nsns.barplot(data=formal_edu.sort_values(by=['edu_order']), x='ratio', y='formal_edu', orient='h')\nplt.title('Ratio of Users - Formal Education', fontsize=15)\nplt.xlabel('ratio')\nprint(\"Most of the people hold Bachelor's or Master's as the highest degree, however, Doctoral degree doesn't occupy that much.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"k_2019 = pd.read_csv(r'/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv')\nk_2018 = pd.read_csv(r'/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv')\n\nds_to_edu_2020 = pd.Series(k_2020.loc[lambda x: x['Q5'] == 'Data Scientist']['Q4'].values[1:]).value_counts(normalize=True).reset_index().rename(columns={'index':'formal_edu', 0:'ratio'}).iloc[:3, :]\nds_to_edu_2020.loc[:, 'year'] = 2020\nds_to_edu_2019 = pd.Series(k_2019.loc[lambda x: x['Q5'] == 'Data Scientist']['Q4'].values[1:]).value_counts(normalize=True).reset_index().rename(columns={'index':'formal_edu', 0:'ratio'}).iloc[:3, :]\nds_to_edu_2019.loc[:, 'year'] = 2019\nds_to_edu_2018 = pd.Series(k_2018.loc[lambda x: x['Q6'] == 'Data Scientist']['Q4'].values[1:]).value_counts(normalize=True).reset_index().rename(columns={'index':'formal_edu', 0:'ratio'}).iloc[:3, :]\nds_to_edu_2018.loc[:, 'year'] = 2018\n\nds_to_edu_by_year = pd.concat([ds_to_edu_2018, ds_to_edu_2019, ds_to_edu_2020], axis=0)\nds_to_edu_by_year\n\nplt.figure(figsize=(8, 4))\nsns.pointplot(data=ds_to_edu_by_year, x='year', y='ratio', hue='formal_edu')\nplt.title('Ratio of Data Scientists - Year by Formal Education', fontsize=15)\n\ndiff_mb = ds_to_edu_by_year.loc[lambda x: x['formal_edu'].str.contains('Master')]['ratio'].mean() - ds_to_edu_by_year.loc[lambda x: x['formal_edu'].str.contains('Bachelor')]['ratio'].mean()\ndiff_md = ds_to_edu_by_year.loc[lambda x: x['formal_edu'].str.contains('Master')]['ratio'].mean() - ds_to_edu_by_year.loc[lambda x: x['formal_edu'].str.contains('Doctor')]['ratio'].mean()\nprint(\"* Kagglers holding the Master's degree are still the majority of data scientists, on average {:.2%} higher than Bachelor's degree and {:.2%} higher than Doctor's degree.\".format(diff_mb, diff_md))\nprint(\"* The ratio of data scientists holding Master's degrees or the ratio for Doctor's degrees are dropping.\")\n\nratio_b_2018 = ds_to_edu_by_year.loc[lambda x: (x['year'] == 2018) & (x['formal_edu'].str.contains('Bachelor'))]['ratio'].values[0]\nratio_b_2020 = ds_to_edu_by_year.loc[lambda x: (x['year'] == 2020) & (x['formal_edu'].str.contains('Bachelor'))]['ratio'].values[0]\nprint(\"* The ratio for Bachelor's degree is increasing, 2018 was {:.2%} and 2020 was {:.2%}, which is round {:.2%} increase.\".format(ratio_b_2018, ratio_b_2020, ratio_b_2020 - ratio_b_2018))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8, 12))\nplt.title('Ratio of Users - Country', fontsize=15)\ncountry = df['country'].value_counts(normalize=True).reset_index().rename(columns={'index':'country', 'country':'ratio'})\nsns.barplot(data=country, x='ratio', y='country', orient='h')\nprint('* Kagglers spread everywhere across countries, that is truly amazing!')\nprint('* India has the most people.')\nprint('* United States is the 2nd place, having {:.2%} people from United States, not that much.'.format(df.loc[df['country'] == 'United States of America'].shape[0]/df.shape[0]))\nprint('* While I know in China we have so many people actively engaging Kaggle competitions, some of them are even doing Kaggle consulting, it just located number 9.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nplt.title('Ratio of Users - Job Title', fontsize=15)\njt = df['job_title'].value_counts(normalize=True).reset_index().rename(columns={'index':'job_title', 'job_title':'ratio'})\nsns.barplot(data=jt, x='ratio', y='job_title')\nprint('Students take up around 25% of the population.')\nprint('Data Scientists take up around 14% of the population.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nplt.title('Ratio of Users - Coding Experience', fontsize=15)\ncode = df['code_exp'].value_counts(normalize=True).reset_index().rename(columns={'index':'code_exp', 'code_exp':'ratio'})\ncode['exp_order'] = code['code_exp'].replace({'I have never written code':0,\n                                         '< 1 years':0.5,\n                                         '1-2 years':1.5,\n                                         '3-5 years':2.5,\n                                         '5-10 years':3.5,\n                                         '10-20 years':4.5,\n                                         '20+ years':7.5})\nsns.barplot(data=code.sort_values(by=['exp_order']), x='ratio', y='code_exp', orient='h')\nprint('Most people have coding experience, and most of them are between 0 and 5, that golden first 5 years!')\nprint('Since Kaggle users are the portion who are willing to take action and being positive on learning, according to the data, people with 0-5 years coding experience tend to be more active.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The language that Kagglers think important is still 🐍, and it's not just because people use it.\n* #### People using python think python is more important, than people don't use python. but the consistency is no matter if using python, Python, R, and SQL are top 3 recommended language to learn, which means we treat these three important.\n* #### People don't use top3 languages at all still recommend Python, and then comes C++, while R is 5th and SQL is 7th"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"language = pd.Series('|'.join(df['language']).split('|')).value_counts().reset_index().rename(columns={'index':'language', 0:'num_people'})\nlanguage = language.loc[lambda x: x['language'] != '']\nlanguage['ratio'] = language['num_people'] / df['language'].shape[0]\n\nplt.figure(figsize=(10, 5))\nax = sns.barplot(data=language, x='ratio', y='language')\nax.set_title('Ratio of Users - Language', fontsize=15)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"lang_combo = df['language'].value_counts().reset_index().rename(columns={'index':'lang_combo', 'language':'num_people'})\nlang_combo = lang_combo.loc[lambda x: x['lang_combo'] != '']\nlang_combo['ratio'] = lang_combo['num_people'] / df['language'].shape[0]\nlang_combo['num_lang'] = lang_combo['lang_combo'].apply(lambda x: len(x.split('|')))\n\nplt.figure(figsize=(8, 4))\nplt.title('Number of Users - Number of Languages', fontsize=15)\nsns.pointplot(data=lang_combo.groupby(['num_lang'])['num_people'].sum().reset_index(), x='num_lang', y='num_people')\nprint('* There are {:d} out of {:d} users solely using one language.'.format(lang_combo.groupby(['num_lang'])['num_people'].sum().reset_index().loc[lambda x: x['num_lang'] == 1, 'num_people'].values[0], df['language'].shape[0]))\nprint('* There are {:d} out of {:d} users using two languages, which is the highest number'.format(lang_combo.groupby(['num_lang'])['num_people'].sum().reset_index().loc[lambda x: x['num_lang'] == 2, 'num_people'].values[0], df['language'].shape[0]))\nprint('* There are {:d} out of {:d} users using three languages, after which the number of users drastically drops as the number of language increases'.format(lang_combo.groupby(['num_lang'])['num_people'].sum().reset_index().loc[lambda x: x['num_lang'] == 3, 'num_people'].values[0], df['language'].shape[0]))\n\nprint(\"\\nSince most people are using Python as the major language plus a lot of them are using one or two more languages as a combination, let's see what are the top 3 combos for pythoners.\")\n# top combo 2\nprint('* The top3 language combo-2:')\nfor i, val in enumerate(lang_combo.loc[lambda x: x['lang_combo'].str.contains('Python')].loc[lambda x: x['num_lang'] == 2].iloc[:3]['lang_combo'].values):\n    print('   {:d}. {:s}'.format(i + 1, val))\n\n# top combo 3\nprint('* The top3 language combo-3:')\nfor i, val in enumerate(lang_combo.loc[lambda x: x['lang_combo'].str.contains('Python')].loc[lambda x: x['num_lang'] == 3].iloc[:3]['lang_combo'].values):\n    print('   {:d}. {:s}'.format(i + 1, val))\nprint('It does show clear ds tracks: Analyst/Scientist, Developer, and Engineer? And needless to say, Python+SQL is the golden combo.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rec_py = pd.DataFrame([['Python', 0], ['no Python but SQL or R', 0], ['none of Python SQL or R', 0]], columns=['lang', 'ratio'])\nrec_py.loc[lambda x: x['lang'] == 'Python', 'ratio'] = df.loc[df['language'].str.contains('Python')]['rec_learn'].value_counts(normalize=True).reset_index().loc[lambda x: x['index'] == 'Python', 'rec_learn'].values[0]\nrec_py.loc[lambda x: x['lang'] == 'no Python but SQL or R', 'ratio'] = df.loc[(df['language'].str.contains('SQL|R')) & (~df['language'].str.contains('Python'))]['rec_learn'].value_counts(normalize=True).reset_index().loc[lambda x: x['index'] == 'Python', 'rec_learn'].values[0]\nrec_py.loc[lambda x: x['lang'] == 'none of Python SQL or R', 'ratio'] = df.loc[~df['language'].str.contains('Python|SQL|R')]['rec_learn'].value_counts(normalize=True).reset_index().loc[lambda x: x['index'] == 'Python', 'rec_learn'].values[0]\n\nplt.figure(figsize=(10, 4))\nax = sns.barplot(data=rec_py, x='lang', y='ratio')\nplt.title('Language People Use - Ratio of People Recommend Python to Aspiring DS', fontsize=15)\nplt.xlabel('langugage people use')\nplt.ylabel('ratio of people rec python')\nfor i, ratio in enumerate(rec_py['ratio'].values):\n    plt.text(x=(i - 0.15), y=0.89 * ratio, s=str(np.round(ratio * 100, 2)) + '%', color='white', fontfamily='fantasy', fontweight='bold', fontsize=15)\n\nprint('Analytics who use other analytical languages other than Python do not think Python very important comparing people not doing analytics at all.')\nprint('* Analytics may think Python can be replaced by other languages when doing data science work.')\nprint('* Non-analytics may think Python is very important when doing data science work.')\nprint('* Python is the good language to bridge gaps among different tracks and areas.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Working Environment Setup and Preference\n* Budget seems to be a big consideration about using host notebooks, top3: Colab, Kaggle, JupyterHub they all have free working space. 5170 leave the question empty, possibly using local tools.\n* For Colab, Kaggle, Google, and Amazon, number/ratio of users shifts across code experience and varies among different job titles."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ide = pd.Series('|'.join(df['ide'].values).split('|')).value_counts(normalize=True).reset_index().rename(columns={'index':'ide', 0:'ratio'})\nide = ide.loc[lambda x: (x['ide'] != '') & (x['ide'] != 'None')]\n\nplt.figure(figsize=(8, 4))\nplt.title('Ratio of Users - IDE', fontsize=15)\nax = sns.barplot(data=ide, x='ide', y='ratio')\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)\nprint('Top 3 IDEs: Jupyter | Visual Studio Code | PyCharm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"notebook = pd.Series('|'.join(df['notebook'].values).split('|')).value_counts().reset_index().rename(columns={'index':'notebook', 0:'ratio'})\nnotebook = notebook.loc[lambda x: (x['notebook'] != '') & (x['notebook'] != 'None')]\n\nplt.figure(figsize=(8, 4))\nplt.title('Ratio of Users - Notebook Platform', fontsize=15)\nax = sns.barplot(data=notebook, x='notebook', y='ratio')\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)\nprint('Top 3 notebook platforms: Colab | Kaggle | JupyterHub')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"notebook_by_job = df.groupby(['job_title']).apply(lambda x: pd.Series('|'.join(x['notebook'].values).split('|')).value_counts()/x['notebook'].shape[0]).reset_index()\nnotebook_by_job.rename(columns={'level_1':'notebook', 0:'ingroup_ratio'}, inplace=True)\n\nnotebook_by_codeexp = df.groupby(['code_exp']).apply(lambda x: pd.Series('|'.join(x['notebook'].values).split('|')).value_counts()/x['notebook'].shape[0]).reset_index()\nnotebook_by_codeexp.rename(columns={'level_1':'notebook', 0:'ingroup_ratio'}, inplace=True)\nnotebook_by_codeexp['order'] = notebook_by_codeexp['code_exp'].replace({'I have never written code': 0, \n                                                                        '< 1 years': 0.5, \n                                                                        '1-2 years': 1.5,\n                                                                        '3-5 years': 4, \n                                                                        '5-10 years':7.5,\n                                                                        '10-20 years':15, \n                                                                        '20+ years': 25})\nnotebook_by_codeexp = notebook_by_codeexp.sort_values(by=['order'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"nb_to_consider = ['Kaggle Notebooks', 'Colab Notebooks', 'Binder / JupyterHub', 'Google Cloud AI Platform Notebooks', 'Azure Notebooks', \n                  'Google Cloud Datalab Notebooks', 'IBM Watson Studio', 'Amazon Sagemaker Studio', 'Amazon EMR Notebooks']\njt_to_consider = ['Student', 'Business Analyst', 'Data Analyst', 'Data Engineer', 'Data Scientist', \n                  'Machine Learning Engineer', 'Product/Project Manager', 'Research Scientist']\n\nplt.figure(figsize=(15, 5))\nax = sns.barplot(data=notebook_by_job.loc[lambda x: \n                                          (x['notebook'].isin(nb_to_consider)) & \n                                          (x['job_title'].isin(jt_to_consider))], \n                 x='notebook', \n                 y='ingroup_ratio', \n                 hue='job_title')\nax.set_title('Notebook - Job Title', fontsize=20)\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)\nprint('Here, we see that Kaggle and Colab are Machine Learning Engineers favorite, while Data Scientists also like them.')\nprint(\"Amazon Sagemaker doesn't have much ratio of users comparing with Kaggle and Google Colab, maybe it's because Kaggle is more connected with Google products.\")\nprint(\"* Although there aren't many users in the survey, what's interesting is there is a gap between the use preference for students and people who work as ds related jobs.\")\nprint(\"* Only small portion of the students use Amazon Sagemaker, but Data Scientists, Data Engineers, and Machine Learning Engineers are using them.\")\nprint('\\n')\n\nplt.figure(figsize=(15, 5))\nax_1 = sns.barplot(data=notebook_by_codeexp.loc[lambda x: \n                                                (x['notebook'].isin(nb_to_consider))], \n                   x='notebook', \n                   y='ingroup_ratio', \n                   hue='code_exp')\nax_1.set_title('Notebook by Coding Experience', fontsize=20)\nax_1.set_xticklabels(labels=ax_1.get_xticklabels(), rotation=90)\nprint('People with 1-5 years coding experience are most actively using Kaggle and Colab.')\nprint('Great to see that those with 5-10 years are also active on the two platforms becuase they highly taking charge of who to hire in a typical DS team.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"vis = df.copy()\nvis['num_vis'] = vis['lib_visual'].apply(lambda x: len(x.split('|')))\nprint('{:.2%} are using at least two different visualization packages.'.format(vis.loc[vis['num_vis'] > 1].shape[0]/df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"vis = pd.Series('|'.join(df['lib_visual'].values).split('|')).value_counts(dropna=False).reset_index()\nvis.columns = ['package', 'frequency']\nvis['ratio'] = vis['frequency']/vis['frequency'].sum()\nvis.sort_values(by=['ratio'], ascending=False)\n\nplt.figure(figsize=(8, 4))\nax = sns.barplot(data=vis.loc[(vis['package'] != '') & (vis['package'] != 'None')], x='package', y='ratio')\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)\nprint('Top 4 visualization packages: Matplotlib | Seaborn | Plotly | ggplot')\nprint('Some less familiar packages: Bokeh | Geoplotlib | D3 js | Leaflet/Folium | Altair')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"vis_by_job = df.groupby(['job_title']).apply(lambda x: pd.Series('|'.join(x['lib_visual'].values).split('|')).value_counts()/x['lib_visual'].shape[0]).reset_index()\nvis_by_job.rename(columns={'level_1':'lib_visual', 0:'ingroup_ratio'}, inplace=True)\n\nplt.figure(figsize=(15, 5))\nax = sns.barplot(data=vis_by_job.loc[lambda x: (x['job_title'].isin(jt_to_consider)) & \n                                               (~x['lib_visual'].isin(['', 'None']))], \n                 x='lib_visual', \n                 y='ingroup_ratio', \n                 hue='job_title')\nax.set_title('Visualization Package by Job Title', fontsize=20)\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90);\nprint('Machine learning engineers love Matplotlib most, maybe it is easier to be integrated as part of the analytical pipeline?')\nprint('Data Scientist has no.1 usage on the following packages: Seaborn | Ggplot | Plotly | Shiny | Bokeh | Leaflet/Folium, and these packages have things in common: easthetical, easy-to-use, interactive')\nprint('Especially for plotly, comparing with other jobs titles, data scientists have more than 10% of people using it.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ml = df.copy().loc[lambda x: ~x['ml_exp'].isnull()]\nml['num_ml_exp'] = ml['ml_exp'].replace({'I do not use machine learning methods':0,\n                                         'Under 1 year':0.5,\n                                         '1-2 years':1.5,\n                                         '2-3 years':2.5,\n                                         '3-4 years':3.5,\n                                         '4-5 years':4.5,\n                                         '5-10 years':7.5,\n                                         '10-20 years':15,\n                                         '20 or more years':25})\nml['num_ml_framework'] = ml['ml_framework'].apply(lambda x: len(x.split('|')))\nml['num_ml_alg'] = ml['ml_alg'].apply(lambda x: len(x.split('|')))\nml['num_cv'] = ml['cv'].apply(lambda x: len(x.split('|')))\nml['num_nlp'] = ml['nlp'].apply(lambda x: len(x.split('|')))\n\n# people only select None or nothing should have 0 number \nml.loc[lambda x: (x['ml_framework'] == 'None') | (x['ml_alg'] == ''), 'num_ml_framework'] = 0\nml.loc[lambda x: (x['ml_alg'] == 'None') | (x['ml_alg'] == ''), 'num_ml_alg'] = 0\nml.loc[lambda x: (x['cv'] == 'None') | (x['cv'] == ''), 'num_cv'] = 0\nml.loc[lambda x: (x['nlp'] == 'None') | (x['nlp'] == ''), 'num_nlp'] = 0\n\nplt.figure(figsize=(10, 6))\nax = sns.lineplot(data=ml.sort_values(by=['num_ml_exp']), x='num_ml_exp', y='num_ml_framework', label='ml_framework')\nax = sns.lineplot(data=ml.sort_values(by=['num_ml_exp']), x='num_ml_exp', y='num_ml_alg', label='ml_alg')\nax = sns.lineplot(data=ml.loc[~ml['cv'].isin(['', None])].sort_values(by=['num_ml_exp']), x='num_ml_exp', y='num_cv', label='cv')\nax = sns.lineplot(data=ml.loc[~ml['nlp'].isin(['', None])].sort_values(by=['num_ml_exp']), x='num_ml_exp', y='num_nlp', label='nlp')\nplt.title('Machine Learning Experience - Number of Skills')\nplt.ylabel('num_skills')\nplt.xticks([0, 0.5, 1.5, 2.5, 3.5, 4.5, 7.5, 15, 25], ml.sort_values(by=['num_ml_exp'])['ml_exp'].unique().tolist(), rotation=90)\nprint('Here, we can see the correlation between number of ml experience to number of ml frameworks and algorithms they reagularly use:')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_ratio_of_people(df, col):\n    freq = pd.Series('|'.join(df.loc[~df[col].isin(['', 'None']), col].astype(str).tolist()).split('|')).value_counts().reset_index().rename(columns={'index':col, 0:'freq'})\n    output = freq.sort_values(by=['freq'], ascending=False)\n    output['ratio'] = output['freq']/df.shape[0]\n    return output\n\nfor col in ['ml_framework', 'ml_alg', 'cv', 'nlp']: \n    data = ml.groupby(['num_ml_exp', 'ml_exp']).apply(lambda x: get_ratio_of_people(x, col=col)).reset_index().loc[lambda x: ~x[col].isin(['', 'None'])]\n    ax = sns.lmplot(data=data, x='num_ml_exp', y='ratio', hue=col, ci=None, order=2, truncate=True, size=5); \n    plt.xlim((0, 26))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's check out some job responsibilities at work 💻📁📊📝. We can categorize them into the following three by universality 🌐:\n* Top1: Analytics 📈📊. Common tasks are Exploratory Data Analysis, Data Mining, Data Manipulation, Data Cleaning, Data Processing, Creating Business Insights, and Visualization\n* Top2: Machine Learning R&D 🔩⚙️. Common tasks are exploring ML applications, building up ML frameworks and POCs, and algorithms optimization\n* Top3: Pipeline/Archetecture 🚰🕸️. Common tasks are building up Data Infrastructure and Pipeline using Platforms like AWS, GCP"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"jd = pd.Series('|'.join(df['jd']).split('|')).value_counts(normalize=True).reset_index().rename(columns={'index':'jd', 0:'ratio'})\nplt.figure(figsize=(8, 4))\nsns.barplot(data=jd.loc[~jd['jd'].isin([''])], y='jd', x='ratio')\nplt.title('Job Responsibility - Ratio of Users', fontsize=15)\nprint('The most common job responsibility for ds-related jobs is analyze and understand data to drive thinking.')\nprint('The 2nd common job responsibility is accomplish Proof of Concepts in new aras by applying machine learning.')\nprint('The 3rd common job responsibility is build and run data manipulation infrastructure/pipeline.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's see what the following table tell us 🙊:\n* For most data-related jobs, they have specific one job responsibility which is clearly more important than the rest. \n* However, data scientists, machine learning engineers, and research scientists don't have one which is apparently more important than the others, rather, they are taking multiple job responsibilities at the same time 💪🏼💪🏼💪🏼. Well, they are better expected to be the 🦸 knowing how to 🏋 projects end2end!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"jd_by_jt = df.groupby(['job_title']).apply(lambda x: pd.Series('|'.join(x['jd']).split('|')).value_counts().reset_index())\njd_by_jt = jd_by_jt.reset_index().rename(columns={'index':'jd', 0:'freq'}).drop(columns=['level_1'])\njd_by_jt = jd_by_jt.merge(jd_by_jt.groupby(['job_title'])['freq'].sum().reset_index().rename(columns={'freq':'total'}), on=['job_title'])\njd_by_jt['ingroup_ratio'] = np.round(jd_by_jt['freq']/jd_by_jt['total'], 3)\n\nplt.figure(figsize=(15, 8))\nax = sns.barplot(data=jd_by_jt.loc[lambda x: \n                                          (x['job_title'].isin(jt_to_consider)) & \n                                          (x['jd'] != '') &\n                                          (~x['jd'].isin(['None of these activities are an important part of my role at work', 'Other']))], \n                 y='jd', \n                 x='ingroup_ratio', \n                 hue='job_title', \n                 orient='h')\nax.set_title('Job Resposibility - Ratio of Users Per Job Title', fontsize=20)\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# User's Costs 💵 on ML & Cloud Services\n* Data scientists 👩‍🔬👨‍🔬 stand out to be the group who have the strongest willingness to put money on ML & Cloud services: more than 25% of people spent 1-999💰, and another similar amount of people spent 10000+💰 in the last 5 years. I would doubt why they spent so much as data scientist maybe they are providing end-to-end service? Or they are building something huge by themselves for themselves? Or they just ... rich? 🤔🤔🤔 (p.s. just a gentle reminder here never forget to stop all the instances whenever you finish using the services, otherwise your money will blow away 💸💸💸)\n* The cost on machine learning and cloud services is positively correlated to the company size, the data science team size, and the company's stage of machine learning developements.\n* Data engineers, machine learning engineers, and project/product managers are also willing to spend money but a large group of them just make some small plays like spending 1-999💰 in the last 5 years. \n* In general, I would rank the spending power like this: data scientists > data engineers > machine learning engineers = project/product managers > research scientists > data analysts > business analysts"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"cloud = df.copy().loc[lambda x: x['job_title'].isin(jt_to_consider)]\ncloud['jt_cat'] = cloud['job_title'].apply(lambda x: 'student' if x == 'Student' else 'experienced')\ncloud.loc[cloud['ml_cost_prev_5_year'] == '$0 ($USD)', 'cost_cat'] = 'no cost ($0)'\ncloud.loc[cloud['ml_cost_prev_5_year'].isin(['$1-$99', '$100-$999']), 'cost_cat'] = 'small cost ($1-999)'\ncloud.loc[cloud['ml_cost_prev_5_year'] == '$1000-$9,999', 'cost_cat'] = 'medium cost ($1000-9999)'\ncloud.loc[cloud['ml_cost_prev_5_year'].isin(['$10,000-$99,999', '$100,000 or more ($USD)']), 'cost_cat'] = 'large cost ($10000+)'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cost_by_jt = cloud.groupby(['job_title']).apply(lambda x: x['cost_cat'].value_counts(normalize=True)).reset_index().rename(columns={'level_1':'cost_cat', 'cost_cat':'ingroup_ratio'})\n\nplt.figure(figsize=(12, 6))\nax = sns.barplot(data=cost_by_jt, y='job_title', x='ingroup_ratio', hue='cost_cat', orient='h')\nax.set_title(\"Job Title - Ratio of Users Per Costs Level on ML & Cloud Services\", fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# ['employer_size', 'bus_ds_size', 'employer_ml_stage']\n\n# employer size: scale\ncloud.loc[lambda x: x['employer_size'].isin(['0-49 employees', '50-249 employees']), 'employer_size_cat'] = 'small'\ncloud.loc[lambda x: x['employer_size'].isin(['250-999 employees']), 'employer_size_cat'] = 'medium'\ncloud.loc[lambda x: x['employer_size'].isin(['1000-9,999 employees', '10,000 or more employees']), 'employer_size_cat'] = 'large'\n\n# employer business ds team size: budgeting and growing potential\ncloud.loc[lambda x: x['bus_ds_size'].isin(['0', np.nan]), 'bus_ds_size_cat'] = 'no'\ncloud.loc[lambda x: x['bus_ds_size'].isin(['1-2', '3-4', '5-9']), 'bus_ds_size_cat'] = 'small'\ncloud.loc[lambda x: x['bus_ds_size'].isin(['10-14']), 'bus_ds_size_cat'] = 'medium'\ncloud.loc[lambda x: x['bus_ds_size'].isin(['15-19', '20+']), 'bus_ds_size_cat'] = 'large'\n\n# employer ml stage: ds developing stage \ncloud.loc[lambda x: x['employer_ml_stage'].isin([np.nan, 'No (we do not use ML methods)']), 'employer_ml_stage_cat'] = 'no'\ncloud.loc[lambda x: x['employer_ml_stage'].isin(['We use ML methods for generating insights (but do not put working models into production)']), 'employer_ml_stage_cat'] = 'init'\ncloud.loc[lambda x: x['employer_ml_stage'].isin(['We are exploring ML methods (and may one day put a model into production)', 'We recently started using ML methods (i.e., models in production for less than 2 years)']), 'employer_ml_stage_cat'] = 'grow'\ncloud.loc[lambda x: x['employer_ml_stage'].isin(['We have well established ML methods (i.e., models in production for more than 2 years)']), 'employer_ml_stage_cat'] = 'devoped'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"col = 'employer_size_cat'\ncost_by_size = cloud.groupby([col]).apply(lambda x: x['cost_cat'].value_counts(normalize=True)).reset_index().rename(columns={'level_1':'cost_cat', 'cost_cat':'ingroup_ratio'})\ncost_by_size['size_order'] = cost_by_size[col].replace({'small':1, 'medium':2, 'large':3})\n\nsns.lmplot(data=cost_by_size, x='size_order', y='ingroup_ratio', hue='cost_cat', ci=None, order=2, truncate=True, size=5); \nplt.title('User Cost Level on ML & Cloud Service by Employer DS Size', fontsize=15)\nax.set_title('User Cost Level on ML & Cloud Service by Employer DS Size')\nplt.xticks(np.arange(1, 4), ['small', 'medium', 'large'], rotation=90)\nplt.xlim((0.8, 3.2))\nplt.xlabel(col);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"col = 'bus_ds_size_cat'\ncost_by_size = cloud.groupby([col]).apply(lambda x: x['cost_cat'].value_counts(normalize=True)).reset_index().rename(columns={'level_1':'cost_cat', 'cost_cat':'ingroup_ratio'})\ncost_by_size['size_order'] = cost_by_size['bus_ds_size_cat'].replace({'no':0, 'small':1, 'medium':2, 'large':3})\n\nsns.lmplot(data=cost_by_size, x='size_order', y='ingroup_ratio', hue='cost_cat', ci=None, order=2, truncate=True, size=5); \nplt.title('User Cost Level on ML & Cloud Service by Business DS Size', fontsize=15)\nplt.xticks(np.arange(4), ['no', 'small', 'medium', 'large'], rotation=90)\nplt.xlim((-0.2, 4.2))\nplt.xlabel(col);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"col = 'employer_ml_stage_cat'\ncost_by_stage = cloud.groupby([col]).apply(lambda x: x['cost_cat'].value_counts(normalize=True)).reset_index().rename(columns={'level_1':'cost_cat', 'cost_cat':'ingroup_ratio'})\ncost_by_stage['size_order'] = cost_by_stage['employer_ml_stage_cat'].replace({'no':0, 'init':1, 'grow':2, 'devoped':3})\n\nsns.lmplot(data=cost_by_stage, x='size_order', y='ingroup_ratio', hue='cost_cat', ci=None, order=2, truncate=True, size=5); \nplt.title('User Cost Level on ML & Cloud Service by Employer ML Stage', fontsize=15)\nplt.xticks(np.arange(4), ['no', 'init', 'grow', 'devoped'], rotation=90)\nplt.xlim((-0.2, 4.2))\nplt.xlabel(col);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sharing work and projects to the public is really good for the community because it not only helps others understand and learn your ideas but you to get feedbacks from people in various of backgrounds. Kagglers are doing it on Kaggle 🥇! And when we dig a little more, we found:\n* In general, Machine learning engineers, data scientists, and data engineers have higher willingness to share work on platforms 😊😊😊\n* Students and the unemployeed don't share work to any platform at all\n* More than 50% of Kagglers don't share their work\n* Top1 share platform is Github"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"share = df.copy()\nshare.loc[share['share_plfm'].isin(['', 'I do not share my work publicly']), 'num_share_plfm'] = 0\nshare.loc[~share['share_plfm'].isin(['', 'I do not share my work publicly']), 'num_share_plfm'] = share.loc[~share['share_plfm'].isin(['', 'I do not share my work publicly']), 'share_plfm'].apply(lambda x: len(x.split('|')))\nprint('Machine Learning Engineers, Data Scientists, and Data Engineers have larger ratio of people who share their work on public platforms.')\nprint('{}({:.2%}) of Machine Learning Engineers, {}({:.2%}) of Data Scientists, and {}({:.2%}) of Data Engineers.'.format(share.loc[(share['job_title'] == 'Machine Learning Engineer') & (share['num_share_plfm'] >= 1)].shape[0],\n                                                                                                            share.loc[(share['job_title'] == 'Machine Learning Engineer') & (share['num_share_plfm'] >= 1)].shape[0]/share.loc[(share['job_title'] == 'Machine Learning Engineer')].shape[0],\n                                                                                                            share.loc[(share['job_title'] == 'Data Scientist') & (share['num_share_plfm'] >= 1)].shape[0], \n                                                                                                            share.loc[(share['job_title'] == 'Data Scientist') & (share['num_share_plfm'] >= 1)].shape[0]/share.loc[(share['job_title'] == 'Data Scientist')].shape[0],\n                                                                                                            share.loc[(share['job_title'] == 'Data Engineer') & (share['num_share_plfm'] >= 1)].shape[0],\n                                                                                                            share.loc[(share['job_title'] == 'Data Engineer') & (share['num_share_plfm'] >= 1)].shape[0]/share.loc[(share['job_title'] == 'Data Engineer')].shape[0]))\nshare.loc[lambda x: x['job_title'].isin(jt_to_consider)].groupby(['job_title'])['num_share_plfm'].describe(percentiles=[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.995])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"share_plfm = pd.Series('|'.join(df['share_plfm'].values).split('|')).value_counts(normalize=True).reset_index().rename(columns={'index':'share_plfm', 0:'ratio'})\nplt.figure(figsize=(10, 5))\nsns.barplot(data=share_plfm, x='ratio', y='share_plfm')\nplt.title('Ratio of Users - Share Platform', fontsize=20)\nprint(\"More than 50% of people don't share their work to public platforms.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"share_by_jt = share.loc[lambda x: x['num_share_plfm'] < 1]['job_title'].value_counts().reset_index().rename(columns={'index':'job_title', 'job_title':'num_no_share'})\nshare_by_jt = share_by_jt.merge(share.loc[lambda x: x['num_share_plfm'] >= 1]['job_title'].value_counts().reset_index().rename(columns={'index':'job_title', 'job_title':'num_share'}),\n                                on='job_title', \n                                how='outer').fillna(0)\n                             \nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10, 5))\nsns.barplot(data=share_by_jt, x='num_no_share', y='job_title', ax=ax1)\nsns.barplot(data=share_by_jt, x='num_share', y='job_title', ax=ax2)\nax2.set_ylabel('')\nprint(\"Students and people currently not employed doesn't share their work to platforms.\")\nprint(\"Data Scientists is the most active group sharing work.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Which course platform do you like? \n* More than 18% of Kagglers like Coursera the most, followed by Kaggle Learn Courses (12.5%) and Udemy (12%)\n* Around 19% of students don't have online course platform to learn \n* Cloud-certification programs and Fast.ai have the least popularity having only 2.5% of Kagglers using them 😿😿😿"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"course = pd.Series('|'.join(df['ds_course_plfm'].values).split('|')).value_counts(normalize=True).reset_index().rename(columns={'index':'course_plfm', 0:'ratio'}).loc[lambda x: ~x['course_plfm'].isin(['', 'None'])]\n\nplt.figure(figsize=(10, 5))\nsns.barplot(data=course, x='ratio', y='course_plfm', orient='h')\nprint('most people use Coursera to learn.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"course_by_jt = df.copy().loc[lambda x: x['job_title'].isin(jt_to_consider)]\ncourse_by_jt['jt_cat'] = course_by_jt['job_title'].apply(lambda x: 'student' if x == 'Student' else 'experienced')\ncourse_by_jt = course_by_jt.groupby(['jt_cat']).apply(lambda x: pd.Series('|'.join(x['ds_course_plfm']).split('|')).value_counts(dropna=True, normalize=True)).reset_index().rename(columns={'level_1':'ds_course_plfm', 0:'ingroup_ratio'})\n\nplt.figure(figsize=(12, 6))\nax = sns.barplot(data=course_by_jt, y='ds_course_plfm', x='ingroup_ratio', hue='jt_cat', orient='h')\nprint('Experienced people tend to be more active on taking courses on most platforms, except Kaggle Learn Courses and University Courses.')\nprint(\"A larger ratio of students left it blank. Maybe they don't take courses or they aren't confident enough to say they take courses.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For data science media sources, I think Kaggle survey well covered major medias that we use to listen and learn. \n* Kaggle takes the 1st place 😬. But here, it shows that *Kaggle notebooks* and *Kaggle forums* are the two major ways, which are both in the readable and interactive formats\n* Youtube takes the 2nd place. What are the popular channels to follow then?\n* Blogs took the 3rd place. Popular accounts are *Towards Data Science* (everybody knows it 🤣) and *Analytics Vidhya*, and they are both in the readable format\n* What students like more: *Youtube*\n* What the experienced like more: *Kaggle*, *Blogs*, *Twitter*, *Journal Publications*, *Email Newsletters*, *Slack Communities*, *Podcasts*\n* What students and the experienced have the similar like: *Reddit*, *Course Forums*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"media = pd.Series('|'.join(df['ds_media'].values).split('|')).value_counts(normalize=True).reset_index().rename(columns={'index':'ds_media', 0:'ratio'}).loc[lambda x: ~x['ds_media'].isin(['', 'None'])]\n\nplt.figure(figsize=(10, 5))\nplt.title('Ratio of Users - Favorite Data Science Media Source', fontsize=20)\nsns.barplot(data=media, x='ratio', y='ds_media', orient='h');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"media_srce_by_jt = df.copy().loc[lambda x: x['job_title'].isin(jt_to_consider)]\nmedia_srce_by_jt['jt_cat'] = media_srce_by_jt['job_title'].apply(lambda x: 'student' if x == 'Student' else 'experienced')\nmedia_srce_by_jt = media_srce_by_jt.groupby(['jt_cat']).apply(lambda x: pd.Series('|'.join(x['ds_media']).split('|')).value_counts(dropna=True, normalize=True)).reset_index().rename(columns={'level_1':'ds_media', 0:'ingroup_ratio'})\n\nplt.figure(figsize=(12, 6))\nplt.title('Ratio of Experienced and Students - Favorite Data Science Media', fontsize=20)\nax = sns.barplot(data=media_srce_by_jt, y='ds_media', x='ingroup_ratio', hue='jt_cat')\nprint('Experienced people tend to be more active on taking courses on most medias, except Youtube and course forums.')\nprint(\"A larger ratio of students left it blank. Maybe they don't watch ds medias or they sometimes just randomly browsing different channels without realizing whatever they are using.\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}